{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1bb50312-5779-4584-ab5f-e1c296bdcb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext.data.functional import generate_sp_model, load_sp_model, sentencepiece_tokenizer, sentencepiece_numericalizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "11b534d2-1e2f-4653-b032-2ecad68df2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and vocabulary\n",
    "model_path = \"../models/gpt.pth\"\n",
    "tokenizer_path = \"../SentencePiece/transformer.model\"\n",
    "vocab_path = \"../SentencePiece/transformer.vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "81bf08bd-93ad-4e25-8da7-099dd597eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path, encoding='utf-8') as f:\n",
    "        # Iterate through each line in the file\n",
    "        for line in f:\n",
    "            # Accesses the vocab file, splits the line by tab, and gets the first entry (the actual token)\n",
    "            # Yield the token from the first column (split by tab)\n",
    "            yield [line.split(\"\\t\")[0]]\n",
    "\n",
    "# Build a vocabulary from the tokens yielded by the yield_tokens function\n",
    "    # <pad> is a padding token that is added to the end of a sentence to ensure the length of all sequences in a batch is the same\n",
    "    # <sos> signals the \"Start-Of-Sentence\" aka the start of the sequence\n",
    "    # <eos> signal the \"End-Of-Sentence\" aka the end of the sequence\n",
    "    # <unk> \"unknown\" token is used if a token is not contained in the vocab\n",
    "# From torchtext library (build_vocab_from_iterator)\n",
    "# Builds a generator object, that is treated like an iterator\n",
    "vocab = build_vocab_from_iterator(yield_tokens(\"../SentencePiece/transformer.vocab\"), specials=['<cls>', '<pad>', '<eos>', '<unk>'], special_first=True)\n",
    "\n",
    "# Set the default index for unknown tokens to the index of the '<unk>' token\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3046995f-0383-4618-84fe-4f95f8f8602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transform = T.Sequential(\n",
    "    T.SentencePieceTokenizer(tokenizer_path),\n",
    "    T.VocabTransform(vocab),\n",
    "    T.AddToken(vocab['<cls>'], begin=True),\n",
    "    T.Truncate(max_seq_len=254),\n",
    "    T.AddToken(vocab['<eos>'], begin=False),\n",
    "    T.ToTensor(padding_value=vocab['<pad>']),\n",
    "    T.PadTransform(max_length=256, pad_value=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ff31cc4-89d8-4022-af0f-4fe299bc35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture (Transformer) and hyperparameters if needed\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    # Transformer followed by computation\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # Residual connection\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "#         self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C ** -0.5\n",
    "#         wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Defines multi-head attention, given num_heads\n",
    "\"\"\"\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size + 1, n_embd)\n",
    "                                                    \n",
    "        # self.sa_heads = MultiHeadAttention(4, n_embd//4) # Four heads, 8-dim self-attention\n",
    "        # self.ffwd = FeedForward(n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head = n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, output_size)\n",
    " \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device)) # (T, C)\n",
    "        x = tok_emb + pos_emb # (B,T,C) -> broadcasted\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, output_size)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef11f7e5-8e4e-467c-a1e4-7786f84c070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "block_size = 256\n",
    "n_embd = 768\n",
    "n_head = 12\n",
    "n_layer = 2\n",
    "dropout = 0.3\n",
    "nepochs = 100\n",
    "output_size = 2\n",
    "vocab_size = len(vocab)\n",
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2a4861b9-0a02-4c47-aaca-e276783f7ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model = Transformer()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a64a0665-0420-4297-9840-666db8340ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n",
      "tensor([[0.9797, 0.0203]], device='cuda:0')\n",
      "Predicted probabilities: negative\n"
     ]
    }
   ],
   "source": [
    "def predict_single_text(model, text):\n",
    "    tokens = text_transform([text]).to(device)\n",
    "    # tokens = tokens.unsqueeze(0)  # Add batch dimension\n",
    "    print(tokens.shape)\n",
    "    with torch.no_grad():\n",
    "        logits = model(tokens)\n",
    "    logits = logits[:, 0, :]\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    label = torch.argmax(probs, dim  = 1)\n",
    "    print(probs)\n",
    "    if label.item() == 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'positive'\n",
    "\n",
    "# Example usage:\n",
    "text = \"Meryl Streep as Kate, a woman dying of cancer, performs her role admirably. No wonder she was up for an Oscar. In the part she proves that caring and nurturing housewives are just as important as their sisters out in the business world. And the lesson she teaches about life's expectations and their lack of fulfillment as the relationship grows, that is the most important thing she teaches her daughter. We can expect too much of our mates. Realize that there are many slips and forgiveness or understanding are the main ingredients of a happy life. This is a sombre movie and the ending though sad, shows reconciliation between the father and daughter. I give this one a ten.\"\n",
    "\n",
    "\n",
    "output = predict_single_text(model, text)\n",
    "\n",
    "\n",
    "print(\"Predicted probabilities:\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep-learning)",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
