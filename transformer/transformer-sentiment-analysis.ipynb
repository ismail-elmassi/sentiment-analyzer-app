{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:55:00.886142Z",
     "iopub.status.busy": "2024-06-18T18:55:00.884622Z",
     "iopub.status.idle": "2024-06-18T18:55:00.893123Z",
     "shell.execute_reply": "2024-06-18T18:55:00.892162Z",
     "shell.execute_reply.started": "2024-06-18T18:55:00.885869Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/emirkocak/in-depth-series-sentiment-analysis-w-transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext.data.functional import generate_sp_model, load_sp_model, sentencepiece_tokenizer, sentencepiece_numericalizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-18T18:55:01.703434Z",
     "iopub.status.busy": "2024-06-18T18:55:01.703086Z",
     "iopub.status.idle": "2024-06-18T18:55:01.707841Z",
     "shell.execute_reply": "2024-06-18T18:55:01.706902Z",
     "shell.execute_reply.started": "2024-06-18T18:55:01.703406Z"
    }
   },
   "outputs": [],
   "source": [
    "root = \"/kaggle/input/movies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:59:33.877564Z",
     "iopub.status.busy": "2024-06-18T18:59:33.877233Z",
     "iopub.status.idle": "2024-06-18T18:59:33.923252Z",
     "shell.execute_reply": "2024-06-18T18:59:33.922055Z",
     "shell.execute_reply.started": "2024-06-18T18:59:33.877539Z"
    }
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc2 in position 313: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:  \u001b[38;5;66;03m# Adjust 'utf-8' to the actual encoding if different\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f2:\n\u001b[0;32m----> 4\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(f):\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m                 text_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc2 in position 313: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "# Specify the correct encoding (e.g., 'utf-8', 'latin-1', etc.)\n",
    "with open(input_file, encoding='utf-8') as f:  # Adjust 'utf-8' to the actual encoding if different\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f2:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                text_only = \"\".join(line.split(\",\")[:-1])\n",
    "                f2.write(text_only + \"\\n\")\n",
    "            except IndexError:\n",
    "                print(f\"Error processing line {i}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:55:02.539241Z",
     "iopub.status.busy": "2024-06-18T18:55:02.538400Z",
     "iopub.status.idle": "2024-06-18T18:55:02.587153Z",
     "shell.execute_reply": "2024-06-18T18:55:02.586093Z",
     "shell.execute_reply.started": "2024-06-18T18:55:02.539210Z"
    }
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc2 in position 313: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviews.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f2:\n\u001b[0;32m----> 3\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m                 text_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      5\u001b[0m                 filtered \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m|\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mn|;\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text_only\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# Replaces double quotes with a space, and \\n with a space\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc2 in position 313: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(root, \"reviews.csv\")) as f:\n",
    "        with open(os.path.join(\"/kaggle/working\", \"data.txt\"), \"w\") as f2:\n",
    "            for line in f:\n",
    "                text_only = \"\".join(line.split(\",\")[:-1])\n",
    "                filtered = re.sub(r'\\\\|\\\\n|;', ' ', text_only.replace('\"', ' ').replace('\\n', ' ')) # Replaces double quotes with a space, and \\n with a space\n",
    "                # Replaces \\\\, \\\\n, and; with a space\n",
    "                # Replaces HTML codes with real characters\n",
    "                filtered = filtered.replace(' #39;', \"'\")\n",
    "                filtered = filtered.replace(' #38;', \"&\")\n",
    "                filtered = filtered.replace(' #36;', \"$\")\n",
    "                filtered = filtered.replace(' #151;', \"-\")\n",
    "                f2.write(filtered.lower() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:29:38.544143Z",
     "iopub.status.busy": "2024-06-18T18:29:38.543405Z",
     "iopub.status.idle": "2024-06-18T18:30:34.057236Z",
     "shell.execute_reply": "2024-06-18T18:30:34.056273Z",
     "shell.execute_reply.started": "2024-06-18T18:29:38.544108Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/kaggle/working/data.txt --model_prefix=/kaggle/working/transformer --vocab_size=30000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /kaggle/working/data.txt\n",
      "  input_format: \n",
      "  model_prefix: /kaggle/working/transformer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 30000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(319) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(174) LOG(INFO) Loading corpus: /kaggle/working/data.txt\n",
      "trainer_interface.cc(346) LOG(WARNING) Found too long line (5212 > 4192).\n",
      "trainer_interface.cc(348) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(349) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(375) LOG(INFO) Loaded all 48752 sentences\n",
      "trainer_interface.cc(381) LOG(INFO) Skipped 1248 too long sentences.\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(395) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(456) LOG(INFO) all chars count=58552081\n",
      "trainer_interface.cc(467) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(477) LOG(INFO) Alphabet size=48\n",
      "trainer_interface.cc(478) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(510) LOG(INFO) Done! preprocessed 48752 sentences.\n",
      "unigram_model_trainer.cc(138) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(142) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(193) LOG(INFO) Initialized 258232 seed sentencepieces\n",
      "trainer_interface.cc(516) LOG(INFO) Tokenizing input sentences with whitespace: 48752\n",
      "trainer_interface.cc(526) LOG(INFO) Done! 289566\n",
      "unigram_model_trainer.cc(488) LOG(INFO) Using 289566 sentences for EM training\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=95272 obj=10.3904 num_tokens=710501 num_tokens/piece=7.45761\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=80099 obj=8.24402 num_tokens=716484 num_tokens/piece=8.94498\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=60072 obj=8.20341 num_tokens=739320 num_tokens/piece=12.3072\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=60056 obj=8.19669 num_tokens=739903 num_tokens/piece=12.3202\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=45041 obj=8.22354 num_tokens=776821 num_tokens/piece=17.247\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=45041 obj=8.21681 num_tokens=776739 num_tokens/piece=17.2452\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=33780 obj=8.2651 num_tokens=819574 num_tokens/piece=24.2621\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=33779 obj=8.25612 num_tokens=819513 num_tokens/piece=24.261\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=33000 obj=8.25977 num_tokens=822854 num_tokens/piece=24.935\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=33000 obj=8.25885 num_tokens=822859 num_tokens/piece=24.9351\n",
      "trainer_interface.cc(604) LOG(INFO) Saving model: /kaggle/working/transformer.model\n",
      "trainer_interface.cc(615) LOG(INFO) Saving vocabs: /kaggle/working/transformer.vocab\n"
     ]
    }
   ],
   "source": [
    "# Generate the SentencePiece tokenizer\n",
    "# Text tokenizer and detokenizer\n",
    "# It will tokenize words into subpieces instead of words\n",
    "# This function will create a set of subtokens to fit the set vocabulary size\n",
    "# There will always be enough subwords to subtokenize a dataset if you think about it :) -> max 2 length pairs = 26!\n",
    "# Saved in the home directory\n",
    "generate_sp_model(os.path.join(\"/kaggle/working\", \"data.txt\"), vocab_size=30000, model_prefix='/kaggle/working/transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:54:30.094256Z",
     "iopub.status.busy": "2024-06-18T18:54:30.093277Z",
     "iopub.status.idle": "2024-06-18T18:54:30.296823Z",
     "shell.execute_reply": "2024-06-18T18:54:30.295492Z",
     "shell.execute_reply.started": "2024-06-18T18:54:30.094223Z"
    }
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc2 in position 16697: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf)\n\u001b[0;32m---> 30\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mIMDB\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataset))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_dataset\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArticle\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[36], line 6\u001b[0m, in \u001b[0;36mIMDB.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Reads the file into a pandas DataFrame\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreviews.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mArticle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Replaces empty entries with a space\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:766\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc2 in position 16697: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "# Class must inherit from Dataset to use DataLoader from torch\n",
    "class IMDB(Dataset):\n",
    "    def __init__(self):\n",
    "\n",
    "        # Reads the file into a pandas DataFrame\n",
    "        self.df = pd.read_csv(os.path.join(root, \"reviews.csv\"), names=[\"Article\", \"Class\"])\n",
    "\n",
    "        # Replaces empty entries with a space\n",
    "        self.df.fillna('', inplace=True)\n",
    "\n",
    "        self.df['Article'] = self.df['Article'].str.replace(r'\\\\n|\\\\|\\\\r|\\\\r\\\\n|\\n|\"', ' ', regex=True)\n",
    "        self.df['Article'] = self.df['Article'].replace({' #39;': \"'\", ' #38;': \"&\", ' #36;': \"$\", ' #151;': \"-\"}, regex=True)\n",
    "\n",
    "    # To use for DataLoader\n",
    "    def __getitem__(self, index):\n",
    "        text = self.df.loc[index][\"Article\"].lower()\n",
    "        \n",
    "        class_label = self.df.loc[index][\"Class\"]\n",
    "\n",
    "        if class_label == 'positive':\n",
    "            class_index = 1\n",
    "        else:\n",
    "            class_index = 0\n",
    "            \n",
    "        return class_index, text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "train_dataset = IMDB()\n",
    "print(len(train_dataset))\n",
    "print(train_dataset.df.loc[0][\"Article\"])\n",
    "train_dataset.df.loc[0][\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:31:33.407571Z",
     "iopub.status.busy": "2024-06-18T18:31:33.407219Z",
     "iopub.status.idle": "2024-06-18T18:31:33.434476Z",
     "shell.execute_reply": "2024-06-18T18:31:33.433688Z",
     "shell.execute_reply.started": "2024-06-18T18:31:33.407545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split 90% - 10%\n",
    "validation_split = 0.9\n",
    "\n",
    "# Total train examples\n",
    "n_train_examples = int(len(train_dataset) * validation_split)\n",
    "\n",
    "# Total validation examples\n",
    "n_valid_examples = len(train_dataset) - n_train_examples\n",
    "\n",
    "# Splits them based on values provided\n",
    "train_data, valid_data = torch.utils.data.random_split(train_dataset, [n_train_examples, n_valid_examples], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:31:35.427234Z",
     "iopub.status.busy": "2024-06-18T18:31:35.426904Z",
     "iopub.status.idle": "2024-06-18T18:31:35.432797Z",
     "shell.execute_reply": "2024-06-18T18:31:35.431769Z",
     "shell.execute_reply.started": "2024-06-18T18:31:35.427211Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create dataloaders for the training and testing datasets\n",
    "# Dataloaders allow for batching, shuffling\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = dataloader.DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last = True)\n",
    "\n",
    "test_loader = dataloader.DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:31:36.937866Z",
     "iopub.status.busy": "2024-06-18T18:31:36.937099Z",
     "iopub.status.idle": "2024-06-18T18:31:37.146107Z",
     "shell.execute_reply": "2024-06-18T18:31:37.145048Z",
     "shell.execute_reply.started": "2024-06-18T18:31:36.937835Z"
    }
   },
   "outputs": [],
   "source": [
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path, encoding='utf-8') as f:\n",
    "        # Iterate through each line in the file\n",
    "        for line in f:\n",
    "            # Accesses the vocab file, splits the line by tab, and gets the first entry (the actual token)\n",
    "            # Yield the token from the first column (split by tab)\n",
    "            yield [line.split(\"\\t\")[0]]\n",
    "\n",
    "# Build a vocabulary from the tokens yielded by the yield_tokens function\n",
    "    # <pad> is a padding token that is added to the end of a sentence to ensure the length of all sequences in a batch is the same\n",
    "    # <sos> signals the \"Start-Of-Sentence\" aka the start of the sequence\n",
    "    # <eos> signal the \"End-Of-Sentence\" aka the end of the sequence\n",
    "    # <unk> \"unknown\" token is used if a token is not contained in the vocab\n",
    "# From torchtext library (build_vocab_from_iterator)\n",
    "# Builds a generator object, that is treated like an iterator\n",
    "vocab = build_vocab_from_iterator(yield_tokens(\"/kaggle/working/transformer.vocab\"), specials=['<cls>', '<pad>', '<eos>', '<unk>'], special_first=True)\n",
    "\n",
    "# Set the default index for unknown tokens to the index of the '<unk>' token\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:31:37.960842Z",
     "iopub.status.busy": "2024-06-18T18:31:37.960029Z",
     "iopub.status.idle": "2024-06-18T18:31:38.020798Z",
     "shell.execute_reply": "2024-06-18T18:31:38.019887Z",
     "shell.execute_reply.started": "2024-06-18T18:31:37.960811Z"
    }
   },
   "outputs": [],
   "source": [
    "# Maximum sequence length for text inputs\n",
    "max_len = 256\n",
    "\n",
    "# Data transform to turn text into vocab tokens\n",
    "text_transform = T.Sequential(\n",
    "    # Tokenize with pre-existing Tokenizer\n",
    "    T.SentencePieceTokenizer(\"/kaggle/working/transformer.model\"),\n",
    "    # Converts the sentences to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    # Add <sos> at the beginning of each sentence. 1 because the index for <sos> in the vocabulary is 1 as seen in previous section\n",
    "    T.AddToken(vocab['<sos>'], begin=True),\n",
    "    # Crop the sentence if it is longer than the max length minus 2 to accommodate <sos> and <eos> tokens\n",
    "    T.Truncate(max_seq_len=max_len-2),\n",
    "    # Add <eos> at the end of each sentence. 2 because the index for <eos> in the vocabulary is 2 as seen in previous section\n",
    "    T.AddToken(vocab['<eos>'], begin=False),\n",
    "    # Convert the list of lists to a tensor. This will also pad a sentence with the <pad> token if it is shorter than the max length.\n",
    "    # This ensures all sentences are the same length!\n",
    "    T.ToTensor(padding_value=vocab['<pad>']),\n",
    "    # Pad the sequence to ensure it's exactly max_len tokens long\n",
    "    T.PadTransform(max_length=max_len, pad_value=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:31:38.892797Z",
     "iopub.status.busy": "2024-06-18T18:31:38.892109Z",
     "iopub.status.idle": "2024-06-18T18:31:38.901498Z",
     "shell.execute_reply": "2024-06-18T18:31:38.900638Z",
     "shell.execute_reply.started": "2024-06-18T18:31:38.892762Z"
    }
   },
   "outputs": [],
   "source": [
    "class TokenDrop(nn.Module):\n",
    "    \"\"\" For a batch of tokens indices, randomly replace a non-specical token with <pad>.\n",
    "    prob (float): probability of dropping a token\n",
    "    pad_token (int): index for the <pad> token\n",
    "    num_special (int): Number of special tokens, assumed to be at the start of the vocab\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prob=0.1, pad_token=0, num_special=4):\n",
    "        self.prob = prob\n",
    "        self.num_special = num_special\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # Randomly sample a bernoulli distribution with p = prob\n",
    "        # Create a mask where 1 means we will replace that token\n",
    "        # Discrete probability distribution\n",
    "        # Here we want to treat the ones as the indexes to drop\n",
    "        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n",
    "        \n",
    "        # Only replace if the token is not a special token\n",
    "        # Ones or zeros. If cannot drop, 0, if can drop, 1\n",
    "        can_drop = (sample >= self.num_special).long()\n",
    "        # Multiply together to get the corresponding tokens to be dropped and not dropped\n",
    "        # Here, 1 represents drop, 0 represents do not drop\n",
    "        mask = mask * can_drop\n",
    "\n",
    "        # Make a mask of pad_token to use for replacing dropped indices with the pad_token\n",
    "        replace_with = (self.pad_token * torch.ones_like(sample)).long()\n",
    "        \"\"\" Sample is the original sample\n",
    "        The mask indicates what tokens can be replaced (0 to not be replaced, 1 to be replaced)\n",
    "        Replace_with is a list of of the pad_token tokens\n",
    "        Here, (1-mask) creates the complement mask. (now, 0 indicates drop, 1 indicates to not drop)\n",
    "        1-1 = 0, 1-0 = 0\n",
    "        Multiplying by sample, retains the original tokens that are not to be kept, and applies the mask on the sample\n",
    "        Here, mask * replace_with does elementwise multiplication and adds the corresponding pad_token to the tokens replaced\n",
    "        \"\"\"\n",
    "        sample_out = (1 - mask) * sample + mask * replace_with\n",
    "        \n",
    "        return sample_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:31:39.799967Z",
     "iopub.status.busy": "2024-06-18T18:31:39.799144Z",
     "iopub.status.idle": "2024-06-18T18:31:39.806642Z",
     "shell.execute_reply": "2024-06-18T18:31:39.805601Z",
     "shell.execute_reply.started": "2024-06-18T18:31:39.799934Z"
    }
   },
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the device of the input tensor\n",
    "        device = x.device\n",
    "        \n",
    "        # Calculate half of the hidden size\n",
    "        half_dim = self.dim // 2\n",
    "        \n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        \n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:31:40.698904Z",
     "iopub.status.busy": "2024-06-18T18:31:40.698163Z",
     "iopub.status.idle": "2024-06-18T18:31:40.710234Z",
     "shell.execute_reply": "2024-06-18T18:31:40.709287Z",
     "shell.execute_reply.started": "2024-06-18T18:31:40.698868Z"
    }
   },
   "outputs": [],
   "source": [
    "class NanoTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "        This class implements a simplified Transformer model for sequence classification. \n",
    "        It uses an embedding layer for tokens, sinusoidal positional embeddings, \n",
    "        a Transformer, and a Linear layer.\n",
    "        \n",
    "        num_emb: The number of unique tokens in the vocabulary. (vocab_size)\n",
    "        output_size: The size of the output layer (number of classes). (4)\n",
    "        hidden_size: The dimension of the hidden layer in the Transformer block (default: 128)\n",
    "        num_heads: The number of heads in the multi-head attention layer (default: 4).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_emb, output_size, hidden_size=128, num_heads=4):\n",
    "        \n",
    "        # Inherits from nn.Module's attributes\n",
    "        super(NanoTransformer, self).__init__()\n",
    "\n",
    "        # Create an embedding for each token\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size) # (vocab_size, 128)\n",
    "        \n",
    "        # Scaling down the embedding weights\n",
    "        self.embedding.weight.data = 0.001 * self.embedding.weight.data\n",
    "        \n",
    "        # Positional embedding\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "\n",
    "        # Multi-head attention\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads = num_heads, batch_first = True)\n",
    "\n",
    "        # Linear layer\n",
    "        self.mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size), # (batch_size, 128, 128)\n",
    "                                 nn.LayerNorm(hidden_size), # (batch_size, 128, 128)\n",
    "                                 nn.ELU(), # (batch_size, 128, 128)\n",
    "                                 nn.Linear(hidden_size, hidden_size)) # (batch_size, 128, 128)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_size, output_size) # (batch_size, 128, 128)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # batch_size, time_steps\n",
    "        batch_size, l = input_seq.shape # (32, 160)\n",
    "\n",
    "        input_embs = self.embedding(input_seq) # (32, 160) -> (32, 160, 128)\n",
    "        \n",
    "        # Add a unique embedding to each token embedding depending on it's position in the sequence\n",
    "        seq_indx = torch.arange(l) # (160)\n",
    "        \n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, -1).expand(batch_size, l, -1) # (1, 160, 128) -> (32, 160, 128)\n",
    "        \n",
    "        pos_emb = pos_emb.to(device)\n",
    "        \n",
    "        embs = input_embs + pos_emb # (32, 160, 128) + (32, 160, 128)\n",
    "        \n",
    "        \n",
    "        output, attn_map = self.multihead_attn(embs, embs, embs) # (32, 160, 128)\n",
    "        \n",
    "        output = self.mlp(output) # (32, 160, 128)\n",
    "\n",
    "        return self.fc_out(output), attn_map # (32, 160, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:32:27.594967Z",
     "iopub.status.busy": "2024-06-18T18:32:27.594202Z",
     "iopub.status.idle": "2024-06-18T18:32:28.787291Z",
     "shell.execute_reply": "2024-06-18T18:32:28.786319Z",
     "shell.execute_reply.started": "2024-06-18T18:32:27.594934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "nepochs = 20\n",
    "\n",
    "hidden_size = 256\n",
    "\n",
    "output_size = 2\n",
    "\n",
    "num_heads = 4\n",
    "\n",
    "tf_classifier = NanoTransformer(num_emb=len(vocab), output_size=2, hidden_size=hidden_size, num_heads=num_heads).to(device)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optim.Adam(tf_classifier.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Cosine annealing scheduler to decay the learning rate\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=nepochs, eta_min=0)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "td = TokenDrop(prob=0.4)\n",
    "\n",
    "training_loss_list = []\n",
    "test_loss_list = []\n",
    "training_acc_list = []\n",
    "test_acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:32:32.151258Z",
     "iopub.status.busy": "2024-06-18T18:32:32.150669Z",
     "iopub.status.idle": "2024-06-18T18:32:32.157443Z",
     "shell.execute_reply": "2024-06-18T18:32:32.156459Z",
     "shell.execute_reply.started": "2024-06-18T18:32:32.151226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:32:34.175119Z",
     "iopub.status.busy": "2024-06-18T18:32:34.174497Z",
     "iopub.status.idle": "2024-06-18T18:32:34.183670Z",
     "shell.execute_reply": "2024-06-18T18:32:34.182669Z",
     "shell.execute_reply.started": "2024-06-18T18:32:34.175087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-This Model Has 8076546 (Approximately 8 Million) Parameters!\n"
     ]
    }
   ],
   "source": [
    "# Number of model parameters\n",
    "num_model_params = 0\n",
    "for param in tf_classifier.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:34:43.397128Z",
     "iopub.status.busy": "2024-06-18T18:34:43.396764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "evaluating\n",
      "Epoch 1/20\n",
      "Training Accuracy: 50.36%\n",
      "Testing Accuracy: 50.94%\n",
      "training\n",
      "evaluating\n",
      "Epoch 2/20\n",
      "Training Accuracy: 51.47%\n",
      "Testing Accuracy: 62.62%\n",
      "training\n",
      "evaluating\n",
      "Epoch 3/20\n",
      "Training Accuracy: 68.04%\n",
      "Testing Accuracy: 80.71%\n",
      "training\n",
      "evaluating\n",
      "Epoch 4/20\n",
      "Training Accuracy: 80.15%\n",
      "Testing Accuracy: 82.95%\n",
      "training\n",
      "evaluating\n",
      "Epoch 5/20\n",
      "Training Accuracy: 83.25%\n",
      "Testing Accuracy: 84.62%\n",
      "training\n",
      "evaluating\n",
      "Epoch 6/20\n",
      "Training Accuracy: 84.82%\n",
      "Testing Accuracy: 84.76%\n",
      "training\n"
     ]
    }
   ],
   "source": [
    "# Initialize training and testing accuracy lists\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "\n",
    "# Loop over each epoch\n",
    "for epoch in range(nepochs):\n",
    "    \n",
    "    tf_classifier.train()\n",
    "    \n",
    "    print(\"training\")\n",
    "    \n",
    "    train_acc_count = 0\n",
    "    test_acc_count = 0\n",
    "    train_steps = 0\n",
    "\n",
    "    # Loop over each batch in the training dataset\n",
    "    for labels, texts in train_loader:\n",
    "        batch_size = labels.shape[0]\n",
    "        \n",
    "        # Transform the text to tokens and move to the GPU\n",
    "        text_tokens = text_transform(list(texts)).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # TokenDrop\n",
    "        text_tokens = td(text_tokens).to(device)\n",
    "        \n",
    "        # Get the model predictions\n",
    "        pred, _ = tf_classifier(text_tokens)\n",
    "        \n",
    "        # Compute the loss using cross-entropy loss\n",
    "        loss = loss_fn(pred[:, 0, :], labels)\n",
    "        \n",
    "        # Backpropagation and optimization step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         print(loss)\n",
    "        \n",
    "        # Log the training loss\n",
    "        training_loss_list.append(loss.item())\n",
    "        \n",
    "        # Update training accuracy\n",
    "        train_acc_count += (pred[:, 0, :].argmax(1) == labels).sum().item()\n",
    "        train_steps += batch_size\n",
    "    \n",
    "    # Calculate average training accuracy\n",
    "    train_acc = (train_acc_count / train_steps)\n",
    "    training_acc_list.append(train_acc)\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    tf_classifier.eval()\n",
    "    \n",
    "    print(\"evaluating\")\n",
    "    train_acc_count = 0\n",
    "    test_acc_count = 0\n",
    "    test_steps = 0\n",
    "    \n",
    "    # Loop over each batch in the testing dataset\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for labels, texts in test_loader:\n",
    "            batch_size = labels.shape[0]\n",
    "            \n",
    "            # Transform the text to tokens and move to the GPU\n",
    "            text_tokens = text_transform(list(texts)).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get the model predictions\n",
    "            pred, _ = tf_classifier(text_tokens)\n",
    "            \n",
    "            # Compute the loss using cross-entropy loss\n",
    "            loss = loss_fn(pred[:, 0, :], labels)\n",
    "            test_loss_list.append(loss.item())\n",
    "            \n",
    "            # Update testing accuracy\n",
    "            test_acc_count += (pred[:, 0, :].argmax(1) == labels).sum().item()\n",
    "            test_steps += batch_size\n",
    "        \n",
    "        # Calculate average testing accuracy\n",
    "        test_acc = (test_acc_count / test_steps)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "    # Print out the results for this epoch\n",
    "    print(f'Epoch {epoch+1}/{nepochs}')\n",
    "    print(f'Training Accuracy: {train_acc*100:.2f}%')\n",
    "    print(f'Testing Accuracy: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8876438,
     "datasetId": 5234400,
     "sourceId": 8722711,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 8877313,
     "datasetId": 5229156,
     "sourceId": 8723520,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 184224327,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
