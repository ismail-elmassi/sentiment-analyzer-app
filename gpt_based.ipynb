{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82e2d9d8-a4ab-4b34-b4b0-d4cf2b647ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/emirkocak/in-depth-series-sentiment-analysis-w-transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "from torchtext.data.functional import generate_sp_model, load_sp_model, sentencepiece_tokenizer, sentencepiece_numericalizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c4387a-55af-4290-a5ef-b375e3e0ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11690b68-81f8-4c4d-9a74-3d93fa5a5a0f",
   "metadata": {},
   "source": [
    "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\tpositive\n",
    "A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.\tpositive\n",
    "I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I'd laughed at one of Woody's comedies in years (dare I say a decade?). While I've never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.\tpositive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f154dd0-d6ad-41fe-a001-fe2e8063bd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(root, \"imdb.csv\")) as f:\n",
    "        with open(os.path.join(root, \"data.txt\"), \"w\") as f2:\n",
    "            for line in f:\n",
    "                text_only = \"\".join(line.split(\",\")[:-1])\n",
    "                filtered = re.sub(r'\\\\|\\\\n|;', ' ', text_only.replace('\"', ' ').replace('\\n', ' ')) # Replaces double quotes with a space, and \\n with a space\n",
    "                # Replaces \\\\, \\\\n, and; with a space\n",
    "                # Replaces HTML codes with real characters\n",
    "                filtered = filtered.replace(' #39;', \"'\")\n",
    "                filtered = filtered.replace(' #38;', \"&\")\n",
    "                filtered = filtered.replace(' #36;', \"$\")\n",
    "                filtered = filtered.replace(' #151;', \"-\")\n",
    "                f2.write(filtered.lower() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92a637ac-13e2-4ab1-a491-4c461197c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the SentencePiece tokenizer\n",
    "# Text tokenizer and detokenizer\n",
    "# It will tokenize words into subpieces instead of words\n",
    "# This function will create a set of subtokens to fit the set vocabulary size\n",
    "# There will always be enough subwords to subtokenize a dataset if you think about it :) -> max 2 length pairs = 26!\n",
    "# Saved in the home directory\n",
    "# generate_sp_model(os.path.join(root, \"data.txt\"), vocab_size=40000, model_prefix='SentencePiece/transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfebdf1c-e900-4d23-a0a0-51db9e55009d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class must inherit from Dataset to use DataLoader from torch\n",
    "class IMDB(Dataset):\n",
    "    def __init__(self):\n",
    "\n",
    "        # Reads the file into a pandas DataFrame\n",
    "        self.df = pd.read_csv(os.path.join(root, \"imdb.csv\"), names=[\"Article\", \"Class\"])\n",
    "\n",
    "        # Replaces empty entries with a space\n",
    "        self.df.fillna('', inplace=True)\n",
    "\n",
    "        self.df['Article'] = self.df['Article'].str.replace(r'\\\\n|\\\\|\\\\r|\\\\r\\\\n|\\n|\"', ' ', regex=True)\n",
    "        self.df['Article'] = self.df['Article'].replace({' #39;': \"'\", ' #38;': \"&\", ' #36;': \"$\", ' #151;': \"-\"}, regex=True)\n",
    "\n",
    "    # To use for DataLoader\n",
    "    def __getitem__(self, index):\n",
    "        text = self.df.loc[index][\"Article\"].lower()\n",
    "        \n",
    "        class_label = self.df.loc[index][\"Class\"]\n",
    "\n",
    "        if class_label == 'positive':\n",
    "            class_index = 1\n",
    "        else:\n",
    "            class_index = 0\n",
    "            \n",
    "        return class_index, text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "train_dataset = IMDB()\n",
    "print(len(train_dataset))\n",
    "print(train_dataset.df.loc[0][\"Article\"])\n",
    "train_dataset.df.loc[0][\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "931c9e58-7f51-413c-b0cb-b2e6c31f64b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 90% - 10%\n",
    "validation_split = 0.9\n",
    "\n",
    "# Total train examples\n",
    "n_train_examples = int(len(train_dataset) * validation_split)\n",
    "\n",
    "# Total validation examples\n",
    "n_valid_examples = len(train_dataset) - n_train_examples\n",
    "\n",
    "# Splits them based on values provided\n",
    "train_data, valid_data = torch.utils.data.random_split(train_dataset, [n_train_examples, n_valid_examples], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e02b2b08-8aa8-4c06-a430-f1cc74ebd141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for the training and testing datasets\n",
    "# Dataloaders allow for batching, shuffling\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = dataloader.DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last = True)\n",
    "\n",
    "test_loader = dataloader.DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97501fdf-9d69-4233-9c5f-a4d6b1f95b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path, encoding='utf-8') as f:\n",
    "        # Iterate through each line in the file\n",
    "        for line in f:\n",
    "            # Accesses the vocab file, splits the line by tab, and gets the first entry (the actual token)\n",
    "            # Yield the token from the first column (split by tab)\n",
    "            yield [line.split(\"\\t\")[0]]\n",
    "\n",
    "# Build a vocabulary from the tokens yielded by the yield_tokens function\n",
    "    # <pad> is a padding token that is added to the end of a sentence to ensure the length of all sequences in a batch is the same\n",
    "    # <sos> signals the \"Start-Of-Sentence\" aka the start of the sequence\n",
    "    # <eos> signal the \"End-Of-Sentence\" aka the end of the sequence\n",
    "    # <unk> \"unknown\" token is used if a token is not contained in the vocab\n",
    "# From torchtext library (build_vocab_from_iterator)\n",
    "# Builds a generator object, that is treated like an iterator\n",
    "vocab = build_vocab_from_iterator(yield_tokens(\"SentencePiece/transformer.vocab\"), specials=['<pad>', '<sos>', '<eos>', '<unk>'], special_first=True)\n",
    "\n",
    "# Set the default index for unknown tokens to the index of the '<unk>' token\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7983fe1-3e8d-4ff2-8c61-bc5b6aae1c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f806eb2c-1334-46aa-885e-be16bc2f761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length for text inputs\n",
    "max_len = 255\n",
    "\n",
    "# Data transform to turn text into vocab tokens\n",
    "\n",
    "# Takes in a list, converts to a tensor\n",
    "text_transform = T.Sequential(\n",
    "    # Tokeniz with pre-existing Tokenizer\n",
    "    T.SentencePieceTokenizer(\"SentencePiece/transformer.model\"),\n",
    "    ## converts the sentences to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "    # 1 as seen in previous section\n",
    "    T.AddToken(vocab['<sos>'], begin=True),\n",
    "    # Crop the sentance if it is longer than the max length\n",
    "    T.Truncate(max_seq_len=max_len),\n",
    "    ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "    # 2 as seen in previous section\n",
    "    T.AddToken(vocab['<eos>'], begin=False),\n",
    "    # Convert the list of lists to a tensor, this will also\n",
    "    # Pad a sentence with the <pad> token if it is shorter than the max length\n",
    "    # This ensures all sentences are the same length!\n",
    "    T.ToTensor(padding_value=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba6af673-2104-4a56-a341-fb9cb50e9fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defines a single attention head\n",
    "\"\"\"\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # Compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C ** -0.5 # (B,T,C) @ (B,C,T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "750d0abe-0772-47f6-babe-889f8c36c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-attention is the communication - per token level\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), \n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3bd83399-8feb-4dcd-8213-106f85e8b683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    # Transformer followed by computation\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # Residual connection\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cebd54b8-966e-4e33-9607-20d8cd8931cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defines multi-head attention, given num_heads\n",
    "\"\"\"\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66993b44-9c15-401c-978f-537727213054",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size + 1, n_embd)\n",
    "                                                    \n",
    "        # self.sa_heads = MultiHeadAttention(4, n_embd//4) # Four heads, 8-dim self-attention\n",
    "        # self.ffwd = FeedForward(n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head = n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, output_size)\n",
    " \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device)) # (T, C)\n",
    "        x = tok_emb + pos_emb # (B,T,C) -> broadcasted\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, output_size)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "afee4006-ed67-4eaf-b384-2c3f10984938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128 # How many independent sequences will we process in parallel?\n",
    "block_size = 256 # What is the maximum context length for predictions?\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_embd = 216\n",
    "n_head = 6\n",
    "n_layer = 2\n",
    "dropout = 0.2\n",
    "nepochs = 5\n",
    "output_size = 2\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f985c55f-48f2-4d06-abb8-4a523a7ea9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer()\n",
    "\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "training_loss_list = []\n",
    "test_loss_list = []\n",
    "training_acc_list = []\n",
    "test_acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d5cf8f6b-a4c6-4879-b053-e9a2e0071caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "loss: 1.5198699235916138\n",
      "loss: 0.9044010043144226\n",
      "loss: 0.8264427781105042\n",
      "loss: 0.7156519293785095\n",
      "loss: 0.729667603969574\n",
      "loss: 0.7327843308448792\n",
      "loss: 0.7010390162467957\n",
      "loss: 0.707414984703064\n",
      "loss: 0.7007042765617371\n",
      "loss: 0.771452009677887\n",
      "loss: 0.6829575896263123\n",
      "loss: 0.6921123266220093\n",
      "loss: 0.7093319296836853\n",
      "loss: 0.7074466943740845\n",
      "loss: 0.7084627747535706\n",
      "loss: 0.7024402618408203\n",
      "loss: 0.6892814636230469\n",
      "loss: 0.6944125294685364\n",
      "loss: 0.6929066777229309\n",
      "loss: 0.6892671585083008\n",
      "loss: 0.7201948761940002\n",
      "loss: 0.7179557681083679\n",
      "loss: 0.6822150349617004\n",
      "loss: 0.6992281675338745\n",
      "loss: 0.7002179622650146\n",
      "loss: 0.7121288776397705\n",
      "loss: 0.703744113445282\n",
      "loss: 0.72760409116745\n",
      "loss: 0.7070961594581604\n",
      "loss: 0.7073965072631836\n",
      "loss: 0.7118015885353088\n",
      "loss: 0.713896632194519\n",
      "loss: 0.7168049216270447\n",
      "loss: 0.6904895305633545\n",
      "loss: 0.693366289138794\n",
      "loss: 0.696235716342926\n",
      "loss: 0.6987327337265015\n",
      "loss: 0.6838561296463013\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Backpropagation and optimization step\u001b[39;00m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Log the training loss\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.8/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.8/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.8/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop over each epoch\n",
    "for epoch in range(nepochs):\n",
    "    \n",
    "    train_acc = 0\n",
    "    train_steps = 0\n",
    "    model.train()\n",
    "    \n",
    "    print(\"training\")\n",
    "    # Loop over each batch in the training dataset\n",
    "    for labels, texts in train_loader:\n",
    "        \n",
    "        # Transform the text to tokens and move to the GPU\n",
    "        text_tokens = text_transform(list(texts)).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Get the model predictions\n",
    "        logits = model(text_tokens)\n",
    "\n",
    "        # Compute the loss using cross-entropy loss\n",
    "        loss = F.cross_entropy(logits[:, 0, :], labels)\n",
    "        print(\"loss:\", loss.item())\n",
    "        \n",
    "        # Backpropagation and optimization step\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Log the training loss\n",
    "        training_loss_list.append(loss.item())\n",
    "        \n",
    "        # Update training accuracy\n",
    "        train_acc += (logits[:, 0, :].argmax(1) == labels).sum().item()\n",
    "        train_steps += batch_size\n",
    "    \n",
    "    # Calculate average training accuracy\n",
    "    train_acc /= train_steps\n",
    "    training_acc_list.append(train_acc)\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    tf_classifier.eval()\n",
    "    test_acc = 0\n",
    "    test_steps = 0\n",
    "\n",
    "    print(\"evaluating\")\n",
    "    # Loop over each batch in the testing dataset\n",
    "    with torch.no_grad():\n",
    "        for labels, texts in test_loader:\n",
    "            \n",
    "            # Transform the text to tokens and move to the GPU\n",
    "            text_tokens = text_transform(list(texts)).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get the model predictions\n",
    "            logits = model(text_tokens)\n",
    "    \n",
    "            # Compute the loss using cross-entropy loss\n",
    "            loss = F.cross_entropy(logits[:, 0, :], labels)\n",
    "            print(\"loss:\", loss.item())\n",
    "            \n",
    "            # Update testing accuracy\n",
    "            test_acc += (logits[:, 0, :].argmax(1) == labels).sum().item()\n",
    "            test_steps += batch_size\n",
    "        \n",
    "        # Calculate average testing accuracy\n",
    "        test_acc /= test_steps\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "    # Print out the results for this epoch\n",
    "    print(f'Epoch {epoch+1}/{nepochs}')\n",
    "    print(f'Training Accuracy: {train_acc*100:.2f}%')\n",
    "    print(f'Testing Accuracy: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c0335-cd6b-4a14-8973-ff5a250b0196",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # every once in a while evaluate the loss on train and val sets\n",
    "    # if iter % eval_interval == 0:\n",
    "    #     losses = estimate_loss()\n",
    "    #     print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep-learning)",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
